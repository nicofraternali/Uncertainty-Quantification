{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center>\n",
    "\n",
    "# **Uncertainty Quantification**\n",
    "\n",
    "<center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Method:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have two sets of data: a vector $\\mathbf{f} = [f_1, f_2, ..., f_n]$ (the observed values) and a vector $\\mathbf{d} = [d_1, d_2, ..., d_n]$  (some predictor values). You want to find a constant  $k$ such that $f_i \\approx k d_i$ for all  $i$. The least squares method helps us find the \"best\" $k$ by minimizing the sum of the squared differences between the observed $f_i$ and the predicted $k d_i$. This sum is called the error, denoted $S$:\n",
    "\n",
    "$$S = \\sum_{i=1}^n (f_i - k d_i)^2$$\n",
    "\n",
    "\n",
    "Our goal is to choose $k$ to make $S$ as small as possible.\n",
    "\n",
    "### <u>Step 1: Define the Error Function</u>\n",
    "We start by writing the error $S$ as a function of $k$:\n",
    "\n",
    "$$S(k) = \\sum_{i=1}^n (f_i - k d_i)^2$$\n",
    "\n",
    "This is a quadratic function in $k$, and since it’s a sum of squares, it’s always non-negative and has a minimum we can find.\n",
    "\n",
    "\n",
    "\n",
    "### <u>Step 2: Minimize the Error</u>\n",
    "To find the value of $k$ that minimizes $S(k)$, we use calculus. Take the derivative of $S(k)$ with respect to $k$ and set it equal to zero (this finds the critical point, which will be the minimum because $S(k)$ is a parabola opening upwards):\n",
    "\n",
    "$$\\frac{dS}{dk} = 0$$\n",
    "\n",
    "Let’s compute the derivative. Expand the sum:\n",
    "\n",
    "$$S(k) = \\sum_{i=1}^n (f_i - k d_i)^2 = \\sum_{i=1}^n (f_i^2 - 2 k f_i d_i + k^2 d_i^2)$$\n",
    "\n",
    "Now, differentiate term by term with respect to $k$ (treating $f_i$ and $d_i$ as constants):\n",
    "\n",
    "- Derivative of $\\sum f_i^2$: 0 (no $k$ in this term).\n",
    "- Derivative of $\\sum -2 k f_i d_i$: $-2 \\sum f_i d_i$ (since $f_i$ and $d_i$ are constants).\n",
    "- Derivative of $\\sum k^2 d_i^2$: $\\sum 2 k d_i^2$ (since $\\frac{d}{dk} k^2 = 2k$).\n",
    "\n",
    "So:\n",
    "\n",
    "$$\\frac{dS}{dk} = -2 \\sum_{i=1}^n f_i d_i + 2 k \\sum_{i=1}^n d_i^2$$\n",
    "\n",
    "Factor out the 2:\n",
    "\n",
    "$$\\frac{dS}{dk} = 2 \\left( k \\sum_{i=1}^n d_i^2 - \\sum_{i=1}^n f_i d_i \\right)$$\n",
    "\n",
    "Set the derivative equal to zero:\n",
    "\n",
    "$$2 \\left( k \\sum_{i=1}^n d_i^2 - \\sum_{i=1}^n f_i d_i \\right) = 0$$\n",
    "\n",
    "Since 2 isn’t zero, divide through:\n",
    "\n",
    "$$k \\sum_{i=1}^n d_i^2 - \\sum_{i=1}^n f_i d_i = 0$$\n",
    "\n",
    "\n",
    "### <u>Step 3: Solve for $k$</u>\n",
    "Rearrange the equation:\n",
    "\n",
    "$$k \\sum_{i=1}^n d_i^2 = \\sum_{i=1}^n f_i d_i$$\n",
    "\n",
    "Now, solve for $k$:\n",
    "\n",
    "$$k = \\frac{\\sum_{i=1}^n f_i d_i}{\\sum_{i=1}^n d_i^2}$$\n",
    "\n",
    "\n",
    "### <u>Step 4: Vector Notation</u>\n",
    "In linear algebra, we can write this more compactly using vectors. The term $\\sum_{i=1}^n f_i d_i$ is the dot product of $\\mathbf{f}$ and $\\mathbf{d}$, written as $\\mathbf{d}^T \\mathbf{f}$ (assuming column vectors, with $\\mathbf{d}^T$ being the transpose). Similarly, $\\sum_{i=1}^n d_i^2 = \\mathbf{d}^T \\mathbf{d}$, the dot product of $\\mathbf{d}$ with itself. So the expression becomes:\n",
    "\n",
    "$$k = \\frac{\\mathbf{d}^T \\mathbf{f}}{\\mathbf{d}^T \\mathbf{d}}$$\n",
    "\n",
    "or\n",
    "\n",
    "$${k}=([d]^T[d])^{-1}[d]^T\\mathbf{f}$$\n",
    "\n",
    "---\n",
    "\n",
    "### <u>Why This Makes Sense</u>\n",
    "This $k$ is the optimal scalar that makes $k \\mathbf{d}$ as close as possible to $\\mathbf{f}$ in terms of squared error. Intuitively, it’s like a weighted average that balances how well $f_i$ aligns with $d_i$ across all data points, giving more weight where $d_i$ is larger.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
